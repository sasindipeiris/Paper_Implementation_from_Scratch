# CLIP_from_scratch
A full CLIP (Contrastive Language–Image Pretraining) model was developed from scratch, guided by the underlying principles of multimodal learning. CLIP is designed to align natural language and images in a shared embedding space by learning directly from image-caption pairs. The key capability it enables is zero-shot classification, where the model can match unseen images to class descriptions using only natural language prompts, without explicit supervised training on those classes.

# 1. Tokenization and Positional Encoding
The preprocessing of textual data began with the implementation of a custom tokenizer() function. This function encoded input text into sequences of integer tokens and generated attention masks to distinguish real tokens from padding. Padding was applied to ensure that all sequences had a uniform fixed length (e.g., 32 tokens). Special tokens were also added: a start-of-text (SOT) token at the beginning and an end-of-text (EOT) token at the end of the sequence. These boundary markers help the model recognize where a meaningful sentence starts and ends.

To inform the transformer model of the order of tokens, positional encoding was integrated. The sinusoidal positional encoding method was chosen, as originally proposed in the "Attention is All You Need" paper. This approach maps each position in a sequence to a unique vector using sine and cosine functions at varying frequencies. This form of encoding is deterministic and generalizes well to longer sequences. Unlike learned positional embeddings, which are updated through gradient descent and may adapt more specifically to a given dataset, sinusoidal encodings are fixed and inject useful inductive biases about sequence position into the model without increasing parameter count.

# 2. Transformer Building Blocks
With token embeddings and positional information in place, the next step involved constructing the transformer components. An AttentionHead was created to perform self-attention. This module first transformed the input sequence into query (Q), key (K), and value (V) vectors using linear projections. Attention weights were calculated by taking the scaled dot product between queries and keys, followed by a softmax function to ensure they summed to one. These weights were used to compute a weighted sum over the value vectors, allowing each token to gather contextual information from other positions in the sequence.

The MultiHeadAttention module extended this mechanism by running multiple attention heads in parallel. Each head learned to focus on different semantic aspects of the input, and their outputs were concatenated and projected back to the model’s width. This diversity of attention heads allows the transformer to extract richer, more nuanced patterns from the data.

The TransformerEncoder block assembled these components with additional structure. It incorporated layer normalization before each sub-layer, residual (skip) connections to facilitate gradient flow, and a feedforward MLP network with GELU activation. The MLP expanded the width by a factor (typically 4) and then reduced it back, enabling the model to learn complex transformations. This full encoder block was stacked multiple times in both the image and text encoder paths to build depth.

# 3. Text Encoder
The text encoder started by transforming input tokens into continuous vectors using an embedding lookup table. These embeddings were augmented with positional encodings and passed through several layers of the transformer encoder. At the end of this stack, the embedding corresponding to the EOT token was extracted as the representation of the entire sentence. This embedding was projected into a joint embedding space using a learned linear projection matrix and normalized using L2 norm. This normalization ensures that similarity comparisons are based on cosine distance, which is critical for the contrastive objective.

# 4. Image Encoder
The image encoder followed a similar architectural principle, inspired by Vision Transformers (ViT). Input images were divided into non-overlapping patches using a Conv2D layer with kernel size and stride equal to the patch size. Each patch was treated like a "token" and projected into the same dimensional space as text tokens. A learnable classification token ([CLS]) was prepended to the sequence to aggregate the global representation of the image. Sinusoidal positional encodings were added to these patch embeddings, and the sequence was passed through transformer layers identical to those in the text encoder. The output corresponding to the [CLS] token was then projected and normalized into the shared embedding space.

# 5. Clip Model
The overall CLIP model was constructed by combining the ImageEncoder and TextEncoder. During training, for each batch of image-caption pairs, both modalities were encoded into embeddings. Cosine similarities were computed between all image-text pairs in the batch using a dot product of their normalized embeddings. A learnable scalar temperature parameter was introduced to control the concentration of the softmax distribution applied to these similarities. A symmetric contrastive loss was used, consisting of two cross-entropy losses—one treating images as queries against text, and one vice versa. This encourages true image-text pairs to have higher similarity than mismatched ones.

# 6. Training Loop
The model was trained using the FashionMNIST dataset, with each digit label mapped to a natural language description (e.g., "An image of a sandal"). Captions were tokenized, and images were normalized. The training loop involved loading batches of data, computing the loss from the CLIP model, zeroing the gradients, performing backpropagation, and updating parameters using the Adam optimizer. The best-performing model (with the lowest loss) was saved to disk.

# 7. Zero-Shot Evaluation
Zero-shot evaluation was conducted by encoding a predefined list of class descriptions using the text encoder and comparing them to new images from the test set. Similarities were computed between the image and each class caption, and the top-k predictions were selected based on the highest scores. The accuracy was measured by checking if the top prediction matched the correct caption. Images were visualized using matplotlib, and the top 5 predicted class names were displayed along with their confidence scores. This demonstrated the model's ability to classify images using textual descriptions alone—without any additional training—highlighting the strength of contrastive language-image learning.

# 8. Summary
This full implementation of CLIP from scratch demonstrated how transformer-based architectures can align two different modalities—images and text—into a unified embedding space using only contrastive supervision. Sinusoidal positional encodings provided a way to inject sequence order without learnable parameters. Attention mechanisms and residual connections allowed deep stacking of layers while maintaining gradient flow. The symmetric contrastive loss, together with normalized embeddings, ensured that the model could learn semantic alignment. The result is a powerful and flexible model capable of reasoning across vision and language in a zero-shot setting, entirely driven by paired image-text data.



