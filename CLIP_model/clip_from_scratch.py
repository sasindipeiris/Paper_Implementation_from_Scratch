# -*- coding: utf-8 -*-
"""CLIP_from_scratch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fGq_c3fqk2lyE6DJ_Gv2I-i-5-i-aGXN
"""

from google.colab import drive
drive.mount('/content/drive')

# Step 1: Upgrade datasets and fsspec to ensure compatibility
!pip install -U datasets fsspec

# Step 2: Restart the runtime to activate upgrades
import os
os.kill(os.getpid(), 9)

import shutil
import os
from datasets import load_dataset

# Optional: manually clear broken fashion_mnist cache
cache_dir = os.path.expanduser("~/.cache/huggingface/datasets/fashion_mnist")
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)

# Load the dataset fresh
dataset = load_dataset("fashion_mnist")
train_set = dataset["train"]
test_set = dataset["test"]

# Print a sample to confirm it's working
print(train_set[0])

!pip install datasets

"""## Import Libraries and Modules"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as T
from torch.utils.data import Dataset,DataLoader
from datasets import load_dataset
import matplotlib.pyplot as plt
import numpy as np

"""# Positional Embedding

This `PositionalEmbedding` class defines a fixed, **sinusoidal positional encoding** module for transformer models, based on the method introduced in the original *"Attention is All You Need"* paper. Its purpose is to inject **sequence order information** into token embeddings, which transformers cannot infer on their own. The constructor (`__init__`) builds a `(max_seq_length, width)` matrix `pe`, where each row corresponds to a position in the input sequence and each column is filled using alternating **sine and cosine** functions with different frequencies — this ensures that each position has a unique pattern that the model can interpret. Lower dimensions vary slowly to capture global positions, while higher dimensions oscillate faster to capture finer positional differences. These encodings are **not trainable** (they're fixed) and stored as a buffer (so they're part of the model but don't get updated during training). During the forward pass, the module adds the positional encodings to the input `x`, which is expected to be a batch of token embeddings of shape `(batch_size, seq_len, width)`, enriching each token not only with its **semantic meaning** (from the token embedding) but also its **position in the sequence**. This is in contrast to **learned positional embeddings**, which instead use a trainable `nn.Embedding` layer to assign position vectors, but may not generalize as well to sequences longer than those seen during training.
"""

class PositionalEmbedding(nn.Module):
  def __init__(self, width, max_seq_length):
    super().__init__()

    pe=torch.zeros(max_seq_length,width)
    for pos in range(max_seq_length):
      for i in range(width):
        if i%2 == 0:
          pe[pos][i]=np.sin(pos/(10000**(i/width)))
        else:
          pe[pos][i]=np.cos(pos/(10000**((i-1)/width)))
      # Add a batch dimension: shape becomes (1, max_seq_length, width)
      # Register pe as a buffer so it's part of the model but not a parameter (not trained)
      self.register_buffer('pe',pe.unsqueeze(0))

  def forward(self,x):
    # x shape: (batch_size, sequence_length, width)
    # Add positional encodings to input embeddings
    # We slice self.pe to match the input's sequence length

    x=x+self.pe
    return x

"""## Attention Head

In transformer models, each input token is represented as a high-dimensional vector called the **token embedding** (or total embedding), which captures its meaning; in the `AttentionHead` class, this embedding is projected into three smaller vectors: **Query (Q)**, **Key (K)**, and **Value (V)** using learned linear layers. These are used in the **scaled dot-product attention** formula:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

where the dot product between Q and K determines how much attention each token pays to others, scaled by the square root of the key dimension (`head_size`) to prevent overly large values. The result is a weighted sum of the values (V), producing context-aware outputs. The **head size** is a portion of the total embedding size and is used in each individual attention head, allowing the model to focus on different types of relationships in parallel. An optional **attention mask** can be applied to block specific positions (e.g., padding or future tokens), setting their scores to `-inf` so they receive zero weight after softmax. This mechanism enables the model to understand both token meaning and inter-token relationships based on context.
"""

class AttentionHead(nn.Module):
  def __init__(self,width,head_size) :
    super().__init__()
    self.head_size=head_size

    self.query=nn.Linear(width,head_size)
    self.key=nn.Linear(width,head_size)
    self.value=nn.Linear(width,head_size)

  def forward(self,x,mask=None):
    #Obtaining Queries,Keys and Values
    Q=self.query(x)
    K=self.key(x)
    V=self.value(x)

    #Dot Product of Queries and Keys
    attention = Q@K.transpose(-2,-1)

    #scaling
    attention=attention/(self.head_size**0.5)

    #Applying Attention Mask
    if mask is not None:
      attention=attention.masked_fill(mask==0,float("-inf"))

    attention=torch.softmax(attention,dim=-1)
    attention=attention@V
    return attention

"""The `MultiHeadAttention` class extends the idea of a single attention head by running multiple `AttentionHead` modules in parallel, allowing the model to focus on different types of relationships between tokens simultaneously. Each head projects the input embeddings into separate query, key, and value vectors using a smaller `head_size` (which is `width // n_heads`), computes scaled dot-product attention, and produces a context-aware output for each token. The outputs of all heads are then concatenated along the last dimension and passed through a final linear layer (`W_o`) to mix the information and restore the original embedding size (`width`). This multi-head structure enables the model to learn richer and more diverse patterns of interaction between tokens than a single head could capture, improving its ability to understand complex sequences.

"""

class MultiHeadAttention(nn.Module):
  def __init__(self,width,n_heads):
    super().__init__()
    self.head_size=width//n_heads

    self.W_o=nn.Linear(width,width)

    self.heads=nn.ModuleList([AttentionHead(width,self.head_size) for _ in range(n_heads)])

  def forward(self,x,mask=None):
    #combine attention heads
    out=torch.cat([head(x,mask=mask) for head in self.heads],dim=-1)
    out=self.W_o(out)

    return out

"""# Transformer Encoder

The `TransformerEncoder` class represents a single encoder block in a transformer architecture, designed to process a sequence of input embeddings and produce context-aware representations. It consists of two core sublayers: multi-head self-attention and a feedforward neural network (MLP), each wrapped with **layer normalization** and **residual connections** to improve training stability and gradient flow. The self-attention mechanism allows each token to compute dynamic weights over all other tokens in the sequence using learned **query, key, and value projections**, enabling the model to model relationships such as dependency, co-reference, and alignment regardless of distance—this is a major advantage over RNNs or CNNs, which are limited by sequential or fixed-size context windows. By running multiple attention heads in parallel (multi-head attention), the model can capture diverse types of dependencies and patterns across different subspaces of the embedding. The MLP then introduces non-linearity and token-wise transformation capacity, expanding and compressing each embedding to capture more abstract features. Layer normalization before each sublayer ensures consistent feature scaling and accelerates convergence, while residual connections help prevent vanishing gradients and allow the network to refine rather than replace earlier representations. Altogether, this structure enables the encoder block to effectively integrate both global context and local token information, forming the foundation of powerful language models like BERT and GPT.
"""

class TransformerEncoder(nn.Module):
  def __init__(self,width,n_heads,r_mlp=4):
    super().__init__()
    self.width=width
    self.n_heads=n_heads

    #Sub-Layer 1 Normalization
    self.ln1=nn.LayerNorm(width)

    #Multi-Head Attention
    self.mha=MultiHeadAttention(width,n_heads)

    #Sub-Layer 2 Normalization
    self.ln2=nn.LayerNorm(width)

    #Multilayer Perception
    self.mlp=nn.Sequential(
        nn.Linear(
            self.width,self.width*r_mlp
        ),
        nn.GELU(),
        nn.Linear(self.width*r_mlp,self.width)
    )
  def forward(self,x,mask=None):
    #Residual Connection After Sub-Layer 1
    x=x+self.mha(self.ln1(x),mask=mask)

    #Residual Connection After Sub-Layer 2
    x=x+self.mlp(self.ln2(x))

    return x

"""# Tokenizer

The `tokenizer` function performs both **encoding** (text to tensor) and **decoding** (tensor back to text) depending on the `encode` flag, and is designed to work with **fixed-length sequences** of UTF-8 byte-level tokens. In encoding mode, it first adds special start (`chr(2)`) and end (`chr(3)`) tokens to the input string to mark boundaries, then right-pads the string with `chr(0)` (NULL characters) so the total sequence length equals a fixed `max_seq_length` (default 32). It then encodes the padded string into UTF-8 bytes and wraps it in a PyTorch `IntTensor`. Simultaneously, it constructs a binary **attention mask** of the same length, marking real tokens with 1 and padding with 0, using the `nonzero()` method to determine how much of the tensor represents meaningful content. In decoding mode, it takes a previously encoded tensor (typically as `text`), skips the first byte (start token) and the last real token (end token) based on the non-zero positions in the `mask`, and converts the remaining bytes back into characters using `chr()`, reconstructing the original string. The mask is then cleared since it's not needed after decoding. This tokenizer ensures consistent tensor sizes for batching, while preserving text boundaries and enabling the model to differentiate real input from padding during training and inference.
"""

def tokenizer(text,encode=True,mask=None,max_seq_length=32):
  if encode:
    out=chr(2)+text+chr(3) #Adding SOT and EOT tokens
    out=out + "".join([chr(0) for _ in range(max_seq_length-len(out))]) #Adding Padding
    out=torch.IntTensor(list(out.encode("utf-8"))) #Encoding Text
    mask=torch.ones(len(out.nonzero()))
    mask=torch.cat((mask,torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)

  else:
    out=[chr(x) for x in text[1:len(mask.nonzero())-1]]
    out="" .join(out)
    mask=None

  return out,mask

"""# Text Encoder

The `TextEncoder` class is a neural network module that transforms input text into a compact, normalized embedding vector suitable for tasks like classification, retrieval, or multimodal alignment (e.g., matching text with images). It begins by converting tokenized input text into dense vector representations using an `nn.Embedding` layer, which maps each token index to a `width`-dimensional vector. To provide the model with information about word order, these embeddings are enriched with positional encodings using a fixed sinusoidal pattern. The enriched token vectors are then passed through a stack of `n_layers` transformer encoder blocks, each composed of multi-head self-attention and a feedforward MLP sub-layer. These layers allow each token to learn contextual relationships with all other tokens in the sequence, updating their representations accordingly. After processing, the model selects the output vector corresponding to the End-of-Text (EOT) token—representing the entire sentence—and projects it into a lower-dimensional embedding space (`emb_dim`) using a learnable linear projection matrix. Finally, this vector is L2-normalized to ensure all embeddings lie on the same unit sphere, which is especially useful for contrastive learning objectives like those used in CLIP. This design makes the `TextEncoder` a complete pipeline for turning raw text into a meaningful, fixed-size vector that captures both semantics and context.
"""

class TextEncoder(nn.Module):
  def __init__(self,vocab_size,width,max_seq_length,n_heads,n_layers,emb_dim):
    super().__init__()

    self.max_seq_length=max_seq_length #Maximum length of input sequence
    self.encoder_embedding=nn.Embedding(vocab_size,width)#Embedding Table
    self.positional_embedding=PositionalEmbedding(width,max_seq_length)
    self.encoder=nn.ModuleList([TransformerEncoder(width,n_heads) or _ in range(n_layers)])

    #learned proj of image to embed
    self.projection=nn.Parameter(torch.randn(width,emb_dim))

  def forward(self,text,mask=None):
    #Text Embedding
    x=self.encoder_embedding(text)

    #Positional Embedding
    x=self.positional_embedding(x)

    #Transformer Encoder
    for encoder_layer in self.encoder:
      x=encoder_layer(x,mask=mask)

    #Take features from the EOT Embedding
    x=x[torch.arange(text.shape[0]),torch.sub(torch.sum(mask[:,0],dim=1),1)]

    #joint multimodal embedding
    if self.projection is not None:
      x=x@self.projection

    x=x/torch.norm(x,dim=-1,keepdim=True)
    return x

"""## Image Encoder

The ImageEncoder class is a Vision Transformer (ViT)-inspired module that converts an input image into a single, normalized embedding vector that captures the semantic content of the image. This encoder is typically used in multimodal systems (e.g., CLIP) where the image embedding needs to be compared to a text embedding in a shared space. The process starts by dividing the input image into fixed-size, non-overlapping patches using a convolutional layer (self.linear_project). Each patch is projected into a vector of size width, producing a sequence of patch embeddings. A special classification token ([CLS]) — a learnable vector — is prepended to this sequence. This token will eventually store the summary of the entire image. The combined sequence is enriched with positional embeddings that encode spatial location information so the model understands patch order.

This full sequence of tokens (patch embeddings + [CLS] token) is passed through a stack of n_layers TransformerEncoder blocks, where each block contains multi-head self-attention and feedforward layers with normalization and residual connections. Through self-attention, each patch token (and the [CLS] token) can attend to all others, allowing the model to capture global relationships between different image regions. After passing through all encoder layers, the output corresponding to the [CLS] token is extracted, as it now represents the globally informed image-level representation.

To prepare this representation for use in contrastive learning or similarity tasks (e.g., comparing to text embeddings), it is passed through a learnable projection matrix (self.projection), reducing it from width dimensions to a smaller embedding size emb_dim. Finally, the output vector is L2-normalized so that it lies on the unit sphere — a critical step for enabling cosine similarity-based matching, which is common in retrieval and cross-modal learning. This final output is a compact, meaningful, and geometrically consistent representation of the input image, ready to be compared or aligned with other modalities such as language.
"""

class ImageEncoder(nn.Module):
  def __init__(self,width,img_size,patch_size,n_channels,n_layers,n_heads,emb_dim):
    super().__init__()

    assert img_size[0]%patch_size[0]==0 and img_size[1]%patch_size[1]==0,"img_size dimensions must be divisible by patch_size dimensions"
    assert width % n_heads == 0,"width must be divisible by n_heads"

    self.n_patches = (img_size[0]*img_size[1])//(patch_size[0]*patch_size[1])
    self.max_seq_length=self.n_patches+1

    #Patch Embedding
    self.linear_project=nn.Conv2d(n_channels,width,kernel_size=patch_size,stride=patch_size)

    #Classification Token
    self.cls_token=nn.Parameter(torch.randn(1,1,width))

    self.positional_embedding=PositionalEmbedding(width,self.max_seq_length)

    self.encoder=nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])

    #learned proj of image to embed
    self.projection=nn.Parameter(torch.randn(width,emb_dim))

  def forward(self,x):
    #Patch Embedding
    x=self.linear_project(x)
    x=x.flatten(2).transpose(1,2)

    #Positional Embedding
    x=torch.cat((self.cls_token.expand(x.size()[0],-1,-1),x),dim=1)
    x=self.positional_embedding(x)

    #Transformer Encoder
    for encode_layer in self.encoder:
      x=encode_layer(x)

    #Take Class Tokens
    x=x[:,0,:]

    #joint multimodal embedding
    if self.projection is not None:
      x=x@self.projection

    x=x/torch.norm(x,dim=-1,keepdim=True)

    return x

"""# CLIP Model

The CLIP class defines a multimodal neural network that learns to align images and texts in a shared embedding space, enabling tasks such as cross-modal retrieval, where a model can match a caption to the correct image (or vice versa). It integrates two main components: an ImageEncoder, based on the Vision Transformer (ViT), and a TextEncoder, based on a Transformer language model. Both encoders independently process their respective modalities — the ImageEncoder divides an input image into fixed-size patches, embeds them, and uses transformer layers to capture spatial relationships, ultimately producing a single [CLS] vector as the image representation. The TextEncoder tokenizes the input text, adds positional information, and processes it through transformer layers to extract the [EOT] (end-of-text) token’s output as the sentence-level representation. Both output vectors are then projected into the same dimensional space (emb_dim) and L2-normalized, ensuring that they can be compared using cosine similarity.

During the forward pass, the image and text embeddings are first computed and then used to form a pairwise similarity matrix (called logits) by computing dot products between every image and every text in the batch. These dot products are scaled by a learnable temperature parameter (initialized to log(1/0.07)), which controls how sharply the model distinguishes correct from incorrect pairs during training — smaller temperatures make the softmax distribution sharper, leading to more confident predictions. The matrix logits is then used to compute cross-entropy loss in two directions: image-to-text and text-to-image. This is called a symmetric contrastive loss, where the model is simultaneously trained to retrieve the correct text for a given image and the correct image for a given text. The target labels are simply the indices along the diagonal (since each image-text pair in the batch is assumed to match), and both loss terms are averaged to produce the final scalar loss. This loss is then used to update the encoders and temperature parameter via backpropagation. The result is a powerful representation learning framework where semantically similar images and texts are pulled close together in the embedding space, while dissimilar pairs are pushed apart — enabling general-purpose image-text understanding without task-specific fine-tuning.
"""

class CLIP(nn.Module):
    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers):
        super().__init__()

        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim)

        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim)

        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


    def forward(self,image,text, mask=None):
        I_e = self.image_encoder(image)
        T_e = self.text_encoder(text, mask=mask)

        # scaled pairwise cosine similarities [n, n]
        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)

        # symmetric loss function
        labels = torch.arange(logits.shape[0]).to(self.device)

        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)
        loss_t = nn.functional.cross_entropy(logits, labels)

        loss = (loss_i + loss_t) / 2

        return loss

"""# Dataset

The FashionMNIST class is a custom PyTorch dataset that extends the standard Fashion MNIST dataset into a multimodal format by pairing each image with a corresponding natural language caption based on its label. It uses the Hugging Face datasets library to load the image-label data and defines a transform to convert each image into a PyTorch tensor suitable for neural network input. A predefined dictionary maps each of the dataset's 10 numeric labels (such as 0 for t-shirt, 1 for trousers, etc.) to a human-readable sentence like "An image of a t-shirt/top." These captions are then passed through a tokenizer function, which converts the text into token indices and generates a corresponding attention mask that distinguishes actual tokens from padding. The dataset returns a dictionary containing the image tensor, the tokenized caption, and the repeated attention mask (reshaped likely for use in self-attention layers). The class supports switching between the training and test splits and ensures each sample returned is formatted for multimodal learning tasks, such as those in contrastive pretraining frameworks like CLIP, where the goal is to align image and text embeddings in a shared representation space.
"""

class FashionMNIST(Dataset):
    def __init__(self, train=True):
        self.dataset = load_dataset("fashion_mnist")

        self.transform = T.ToTensor()

        if train:
            self.split = "train"
        else:
            self.split = "test"


        self.captions = {0: "An image of a t-shirt/top",
                        1: "An image of trousers",
                        2: "An image of a pullover",
                        3: "An image of a dress",
                        4: "An image of a coat",
                        5: "An image of a sandal",
                        6: "An image of a shirt",
                        7: "An image of a sneaker",
                        8: "An image of a bag",
                        9: "An image of an ankle boot"}


    def __len__(self):
        return self.dataset.num_rows[self.split]

    def __getitem__(self,i):
        img = self.dataset[self.split][i]["image"]
        img = self.transform(img)

        cap, mask = tokenizer(self.captions[self.dataset[self.split][i]["label"]])

        mask = mask.repeat(len(mask),1)

        return {"image": img, "caption": cap, "mask": mask}

"""# Training Parameters"""

emb_dim=32
vit_width=9
img_size=(28,28)
patch_size=(14,14)
n_channels=1
vit_layers=3
vit_heads=3
vocab_size=256
text_width=32
max_seq_length=32
text_heads=8
text_layers=4
lr=1e-3
epochs=10
batch_size=128

"""# Loading Dataset"""

train_set = FashionMNIST(train = True)
test_set = FashionMNIST(train = False)

train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)

print(train_set[0]["image"].shape)
print(train_set[0]["caption"].shape)
print(train_set[0]["mask"].shape)
print(train_set[0]["caption"])
print(train_set[0]["mask"])

"""# Training

This code trains a CLIP (Contrastive Language–Image Pretraining) model using a multimodal dataset consisting of image-caption pairs, such as those from FashionMNIST, where each image is associated with a descriptive natural language sentence. It first determines the appropriate device for computation—GPU if available, or CPU otherwise—and initializes the CLIP model using specified architectural parameters for both the image and text encoders, including transformer widths, number of layers, attention heads, and shared embedding dimension. The model is then moved to the selected device, and the Adam optimizer is initialized to update its parameters. A variable best_loss is used to keep track of the lowest training loss seen so far, which helps in saving the best-performing model during training. The training loop runs for a specified number of epochs, where each epoch iterates over mini-batches of the dataset using a PyTorch DataLoader. In each batch, the image tensors, tokenized captions, and attention masks are moved to the same device as the model. The model computes a contrastive loss, encouraging image and text pairs to have high cosine similarity when they match and low similarity when they don't. Before updating the model's weights, the code calls optimizer.zero_grad() to reset all accumulated gradients from previous iterations—an essential step because PyTorch accumulates gradients by default. Then, loss.backward() performs backpropagation, traversing the computation graph in reverse and calculating gradients of the loss with respect to all model parameters using the chain rule of calculus. These gradients are stored in the .grad attributes of the parameters. Finally, optimizer.step() applies the Adam optimization algorithm to update the model's weights, using the gradients to determine the direction and magnitude of change for each parameter while also adapting learning rates per parameter using internal momentum and variance estimates. After each epoch, the code prints the most recent batch’s loss, and if it’s lower than the best loss recorded so far, it updates best_loss and saves the model’s state to Google Drive. This ensures only the best version of the model is preserved for future use. The complete training process fine-tunes both encoders and the shared embedding space, allowing the model to learn meaningful cross-modal relationships between images and their textual descriptions.
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device: ", device, f"({torch.cuda.get_device_name(device)})" if torch.cuda.is_available() else "")

model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)

optimizer = optim.Adam(model.parameters(), lr=lr)

best_loss = np.inf
for epoch in range(epochs):
    for i, data in enumerate(train_loader, 0):
        img, cap, mask = data["image"].to(device), data["caption"].to(device), data["mask"].to(device)
        loss = model(img,cap,mask)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Saves model if it performed better than the previous best
    print(f"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}")
    if loss.item() <= best_loss:
        best_loss = loss.item()
        torch.save(model.state_dict(), "/content/drive/MyDrive/clip.pt")
        print("Model Saved.")

"""# Testing

This code loads the best-performing CLIP model saved during training and uses it to evaluate the model’s ability to match images with their corresponding captions from the test set, effectively measuring the model’s zero-shot image classification accuracy. First, a new CLIP model is instantiated using the same architecture and hyperparameters as during training, and the saved weights are loaded from a .pt file stored in Google Drive using torch.load, ensuring compatibility with the current device (CPU or GPU). Next, all the possible caption templates from the dataset (e.g., "An image of a shirt", "An image of a sneaker", etc.) are tokenized in advance and stacked into a tensor for batch processing. Corresponding attention masks are also stacked and expanded using .repeat() and .reshape() to ensure they are compatible with the transformer’s multi-head attention mechanism, resulting in a 3D mask of shape [n_captions, seq_len, seq_len].

The evaluation loop then runs without tracking gradients (torch.no_grad()), which reduces memory usage and speeds up inference. For each batch of test images, the model computes their embeddings using the image_encoder, and compares them to the precomputed text embeddings (from all class captions) generated using the text_encoder. Both image and text feature vectors are L2-normalized to ensure that similarity comparisons are based on cosine similarity, which is important for contrastive learning. A similarity matrix is then computed by taking the dot product of image features with the transpose of text features, scaling by 100.0 before applying softmax to interpret them as probabilities across possible classes.

For each image, the caption with the highest similarity score is selected using torch.max, and the corresponding predicted caption is regenerated from the test_set.captions dictionary based on the predicted index. These predicted captions are compared token-by-token to the actual ground-truth tokenized captions from the test data. A prediction is considered correct only if all tokens match, which is a strict metric. The number of correct predictions is accumulated and compared against the total number of examples to compute the overall model accuracy in percentage. This evaluation pipeline essentially treats CLIP as a zero-shot classifier, testing its ability to choose the correct textual description for a given image without having been explicitly trained to classify images in this way.
"""

#Loading the Best Model
model=CLIP(emb_dim,vit_width,img_size,patch_size,n_channels,vit_layers,vit_heads,vocab_size,text_width,max_seq_length,text_heads,text_layers).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/clip.pt",map_location=device))

#Getting dataset captions to compare images to
text=torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device)
mask=torch.stack([tokenizer(x)[1] for x in test_set.captions.values()])
mask=mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)

correct,total=0,0
with torch.no_grad():
  for data in test_loader:
    images,labels=data["image"].to(device),data["caption"].to(device)
    image_features=model.image_encoder(images)
    text_features=model.text_encoder(text,mask=mask)

    image_features /=image_features.norm(dim=-1,keepdim=True)
    text_features /= text_features.norm(dim=-1,keepdim=True)
    similarity=(100.0*(image_features@text_features.T)).softmax(dim=-1)
    _,indices=torch.max(similarity,1)
    pred=torch.stack([tokenizer(test_set.captions[int(i)])[0] for i in indices]).to(device)
    correct+=int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))
    total+=len(labels)

  print(f'\nModel Accuracy : {100*correct//total} %')

"""# Zero-Shot Classification

This code demonstrates how to perform **zero-shot image classification** using a trained CLIP model by comparing the embedding of a test image against embeddings of textual class descriptions, without training the model on any explicit classification labels. It begins by loading the best-performing CLIP model from disk, ensuring it uses the same architecture and pretrained weights as during training. A list of `class_names` representing FashionMNIST categories (like "sneaker", "coat", etc.) is defined and tokenized into numerical form using a tokenizer, with corresponding attention masks generated and reshaped to be compatible with the transformer's multi-head attention mechanism. A specific test image is selected from the dataset, visualized using `matplotlib`, and its actual caption is decoded and displayed. The image and tokenized class names are then encoded through CLIP's `image_encoder` and `text_encoder` respectively to generate their feature vectors. These embeddings are normalized (unit vectors) to allow cosine similarity to be computed via simple dot product. A similarity matrix is calculated between the image embedding and all class caption embeddings, scaled by a factor of 100 for sharper distribution, and passed through a softmax function to produce a probability-like distribution over class labels. The top 5 predictions with highest similarity scores are selected and printed alongside their confidence percentages. This entire process exemplifies how CLIP leverages shared image-text embeddings to perform classification on unseen classes using only their **natural language descriptions**, enabling powerful zero-shot reasoning without traditional label-based training.
"""

# Loading Best Model
model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/clip.pt", map_location=device))


# Captions to compare images to
class_names =["t-shirt/top",
                        "trousers",
                        "pullover",
                        "dress",
                        "coat",
                        "sandal",
                        "shirt",
                        "sneaker",
                        "bag",
                        "ankle boot"]

# class_names =["An image of a t-shirt/top",
#                         "An image of trousers",
#                         "An image of a pullover",
#                         "An image of a dress",
#                         "An image of a coat",
#                         "An image of a sandal",
#                         "An image of a shirt",
#                         "An image of a sneaker",
#                         "An image of a bag",
#                         "An image of an ankle boot"]

text = torch.stack([tokenizer(x)[0] for x in class_names]).to(device)
mask = torch.stack([tokenizer(x)[1] for x in class_names])
mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)

idx = 1000

img = test_set[idx]["image"][None,:]
plt.imshow(  img[0].permute(1, 2, 0)  ,cmap="gray")
plt.title(tokenizer(test_set[idx]["caption"], encode=False, mask=test_set[idx]["mask"][0])[0])
plt.show()
img = img.to(device)
with torch.no_grad():
  image_features = model.image_encoder(img)
  text_features = model.text_encoder(text, mask=mask)


image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
print("\nTop predictions:\n")
for value, index in zip(values, indices):
    print(f"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%")