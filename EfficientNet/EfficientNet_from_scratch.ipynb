{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "9VJNWCodvUcT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Parameter               | Meaning                                                     |\n",
        "| ----------------------- | ----------------------------------------------------------- |\n",
        "| `expansion_ratio` (`t`) | Multiplier for expanding input channels (used in MBConv)    |\n",
        "| `output_channels` (`c`) | Output channels after projection                            |\n",
        "| `num_repeats` (`r`)     | How many times to repeat this block                         |\n",
        "| `stride` (`s`)          | Stride of the first block in this stage (usually 1 or 2)    |\n",
        "| `kernel_size` (`k`)     | Size of the depthwise convolution filter (e.g., 3x3 or 5x5) |\n"
      ],
      "metadata": {
        "id": "3y0Sff55vnXs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CE4ak7fWnYJ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import nn\n",
        "\n",
        "# Each sublist represents one stage in the network\n",
        "#Basic structure before scaling\n",
        "basic_mb_params =[\n",
        "    # t, channels(c), repeats(t), stride(s), kernel_size(k)\n",
        "    [1,16,1,1,3],\n",
        "    [6,24,2,2,3],\n",
        "    [6,40,2,2,5],\n",
        "    [6,80,3,2,3],\n",
        "    [6,112,3,1,5],\n",
        "    [6,192,4,2,5],\n",
        "    [6,320,1,1,3],\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "alpha, beta = 1.2, 1.1  #depth scaling, width scaling\n",
        "\n",
        "scale_values = {\n",
        "    # (phi, resolution, dropout)\n",
        "    \"b0\":(0,224,0.2),\n",
        "    \"b1\":(0.5,240,0.2),\n",
        "    \"b2\":(1,260,0.3),\n",
        "    \"b3\":(2,300,0.3),\n",
        "    \"b4\":(3,380,0.4),\n",
        "    \"b5\":(4,456,0.4),\n",
        "    \"b6\":(5,528,0.5),\n",
        "    \"b7\":(6,600,0.5),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Activation | Best For                    | Behavior Near 0 | Complexity | Key Trait                   |\n",
        "| ---------- | --------------------------- | --------------- | ---------- | --------------------------- |\n",
        "| **ReLU**   | General deep learning       | Linear          | Low        | Fast and simple             |\n",
        "| **SiLU**   | CNNs (e.g., EfficientNet)   | Smooth, soft    | Moderate   | Boosts performance slightly |\n",
        "| **GELU**   | NLP, Transformers           | Smooth, noisy   | High       | Better at handling noise    |\n",
        "| **SinLU**  | Oscillatory or experimental | Oscillatory     | Moderate   | Periodic behavior           |\n"
      ],
      "metadata": {
        "id": "LjiNQzIPIzbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Part                  | Purpose                                                            |\n",
        "| --------------------- | ------------------------------------------------------------------ |\n",
        "| `nn.Conv2d(...)`      | Performs convolution to extract features (edges, textures, etc.)   |\n",
        "| `nn.BatchNorm2d(...)` | Normalizes output of Conv2D for stability and faster convergence   |\n",
        "| `nn.SiLU()`           | Smooth non-linear activation function: $x \\cdot \\text{sigmoid}(x)$ |\n"
      ],
      "metadata": {
        "id": "6GThMeEtytvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Conv:\n",
        "Input (3 channels) → 64 filters (3x3x3) → Output (64 channels)\n",
        "\n",
        "Depthwise Separable Conv:\n",
        "1. Depthwise Conv (3x3 on each input channel separately) → Output (3 channels)\n",
        "2. Pointwise Conv (1x1x3 filters to mix channels) → Output (64 channels)\n"
      ],
      "metadata": {
        "id": "1mioNSUl0O16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.cnnblock=nn.Sequential(\n",
        "                  nn.Conv2d(in_channels, out_channels,kernel_size, stride, padding, groups=groups),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.SiLU())\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.cnnblock(x)\n",
        ""
      ],
      "metadata": {
        "id": "JQZzjlI2S9B8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Parameter      | Meaning                                                                      |\n",
        "| -------------- | ---------------------------------------------------------------------------- |\n",
        "| `in_channels`  | Number of channels in the input tensor                                       |\n",
        "| `out_channels` | Channels after projecting features                                           |\n",
        "| `kernel_size`  | Size of filter in depthwise convolution                                      |\n",
        "| `stride`       | Controls downsampling (usually 1 or 2)                                       |\n",
        "| `padding`      | Keeps spatial size same after conv                                           |\n",
        "| `ratio`        | **Expansion ratio**: determines if input gets expanded before depthwise conv |\n",
        "| `reduction`    | Factor to reduce channels in **Squeeze-and-Excitation (SE)**                 |\n"
      ],
      "metadata": {
        "id": "yAkC93nt1l7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Stage           | Operation             | Purpose                                     |\n",
        "| --------------- | --------------------- | ------------------------------------------- |\n",
        "| Optional        | `expand_conv`         | Increase channel dimension (if needed)      |\n",
        "| Depthwise Conv  | `groups=hidden_dim`   | Filter spatial features per channel         |\n",
        "| SE Block        | `SqueezeExcitation`   | Focus on important channels                 |\n",
        "| Pointwise Conv  | `1x1 conv`            | Mix channel info, reduce to output channels |\n",
        "| BN & Activation | `BatchNorm2d`, `SiLU` | Normalize & activate features               |\n"
      ],
      "metadata": {
        "id": "0VUulVQy2qiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,kernel_size,stride,padding,ratio,reduction=2,):\n",
        "    super(MBBlock, self).__init__()\n",
        "    hidden_dim=in_channels*ratio\n",
        "    self.expand = in_channels != hidden_dim\n",
        "\n",
        "    # This is for squeeze and excitation block\n",
        "    reduced_dim = int(in_channels / reduction)\n",
        "\n",
        "    if self.expand:\n",
        "      self.expand_conv = ConvBlock(in_channels,hidden_dim,kernel_size = 3,stride=1,padding=1)\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        ConvBlock(hidden_dim,hidden_dim,kernel_size,stride,padding,groups=hidden_dim),\n",
        "        SqueezeExcitation(hidden_dim,reduced_dim),\n",
        "        nn.Conv2d(hidden_dim, out_channels, 1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    if self.expand:\n",
        "      x=self.expand_conv(inputs)\n",
        "    else:\n",
        "      x=inputs\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "R6nu8ah8Wv1x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step        | Component                    | What It Does                              |\n",
        "| ----------- | ---------------------------- | ----------------------------------------- |\n",
        "| 1. Squeeze  | `AdaptiveAvgPool2d(1)`       | Global average pooling (reduce H×W → 1×1) |\n",
        "| 2. Excite   | Conv + SiLU + Conv + Sigmoid | Learn importance of each channel          |\n",
        "| 3. Reweight | `x * se(x)`                  | Emphasize important channels              |\n"
      ],
      "metadata": {
        "id": "0X7hASQZ4w9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeExcitation(nn.Module):\n",
        "  def __init__(self,in_channels,reduced_dim):\n",
        "    super(SqueezeExcitation,self).__init__()\n",
        "    self.se=nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        nn.Conv2d(in_channels,reduced_dim,1),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(reduced_dim,in_channels,1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x * self.se(x)"
      ],
      "metadata": {
        "id": "IEeX_ki4iGOx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Section**            | **Purpose**                                                                |\n",
        "| ---------------------- | -------------------------------------------------------------------------- |\n",
        "| Stem ConvBlock         | Extract low-level features & downsample input image.                       |\n",
        "| MBConv Block Loop      | Efficiently capture complex spatial patterns via lightweight convolutions. |\n",
        "| Width/Depth Scaling    | Control model size dynamically across EfficientNet variants.               |\n",
        "| Squeeze-and-Excitation | Add channel-wise attention to focus on important features.                 |\n",
        "| Final Projection Block | Aggregate and project features into a unified high-level representation.   |\n",
        "| `nn.Sequential`        | Bundle all feature layers into one forward pass.                           |\n"
      ],
      "metadata": {
        "id": "LLVT9itMPuYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil"
      ],
      "metadata": {
        "id": "GrM0oiBP70dL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Stage          | What It Does                                                                |\n",
        "| -------------- | --------------------------------------------------------------------------- |\n",
        "| **Stem**       | Initial `ConvBlock` that reduces input size and increases channels          |\n",
        "| **Body**       | Stacked **MBConv** blocks (Mobile Inverted Bottlenecks with SE + expansion) |\n",
        "| **Head**       | 1x1 conv layer to increase channels to a fixed `last_channels` size         |\n",
        "| **Pooling**    | Global average pooling to convert spatial info into vector form             |\n",
        "| **Classifier** | Dropout + linear layer for final prediction                                 |\n"
      ],
      "metadata": {
        "id": "EGtBhlLi97_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNet(nn.Module):\n",
        "  def __init__(self,model_name,output):\n",
        "    super(EfficientNet,self).__init__()\n",
        "    phi,resolution,dropout=scale_values[model_name]\n",
        "    self.depth_factor,self.width_factor=alpha**phi,beta**phi\n",
        "\n",
        "    #Output of the last MBConv block is adjusted to a fixed dimensionality (scaled)\n",
        "    self.last_channels=ceil(1280*self.width_factor)\n",
        "\n",
        "    #Global average pooling compresses [B, C, H, W] → [B, C, 1, 1]\n",
        "    self.avgpool=nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    #Flatten turns it into a vector [B, C] for classification\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(self.last_channels, output),\n",
        "    )\n",
        "\n",
        "\n",
        "   def feature_extractor(self):\n",
        "      channels = int(32 * self.width_factor)\n",
        "      features = [ConvBlock(3,channels,3,stride=2,padding=1)]\n",
        "      in_channels=channels\n",
        "\n",
        "      for k,c_o,repeats,n in basic_mb_params:\n",
        "        out_channels = 4 * ceil(int(c_o * self.width_factor)/4) #Many hardware accelerators (like GPUs or TPUs) are optimized for processing channel counts that are multiples of 4, 8, 16, etc.\n",
        "        num_layers=ceil(repeat * self.depth_factor)\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "          if layer ==0 :\n",
        "            stride = s\n",
        "          else:\n",
        "            stride=1\n",
        "          features.append(\n",
        "              MBBlock(in_channels,out_channels,expand_ratio=k,stride=stride,kernel_size=n,padding=n//2))\n",
        "          in_channels = out_channels\n",
        "\n",
        "        features.append(ConvBlock(in_channels,self.last_channels,kernel_size=1,stride=1,padding=0))\n",
        "        self.extractor = nn.Sequential(*features)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x=self.avgpool(self.extractor(x))\n",
        "      return self.classifier(self.flatten(x))\n",
        "\n",
        "model_name='b1'\n",
        "output_class = 1000 # for imagenet\n",
        "effnet=EfficientNet(model_name,output_class)\n"
      ],
      "metadata": {
        "id": "5dae1OlP5VVh"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}