{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12502275,"sourceType":"datasetVersion","datasetId":7890578},{"sourceId":3980818,"sourceType":"datasetVersion","datasetId":2362246}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:55:55.421992Z","iopub.execute_input":"2025-07-20T10:55:55.422268Z","iopub.status.idle":"2025-07-20T10:56:00.512323Z","shell.execute_reply.started":"2025-07-20T10:55:55.422245Z","shell.execute_reply":"2025-07-20T10:56:00.511348Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# ConvBNAct Block\nConverts our input image (B, 3,640,640) into (B,64, 320,320)","metadata":{}},{"cell_type":"code","source":"class ConvBNAct (nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride,padding, bias = False),\n            nn.BatchNorm2d(out_channels),\n            nn.SiLU())\n    def forward(self, x):\n        return self.block(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:56:04.610475Z","iopub.execute_input":"2025-07-20T10:56:04.611264Z","iopub.status.idle":"2025-07-20T10:56:04.617225Z","shell.execute_reply.started":"2025-07-20T10:56:04.611235Z","shell.execute_reply":"2025-07-20T10:56:04.616236Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# BottleNeck Block\n\nExtracts important features from the image\n\nInput: (B,64,320,320) --> 1×1 conv -> (B,32,320,320) -->  3×3 conv -> (B,64,320,320) --> Residual :if allowed --> out = out + input","metadata":{}},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    def __init__(self,in_channels,out_channels,shortcut = True,expansion =0.5):\n        super().__init__()\n        hidden_channels = int(out_channels * expansion)\n        self.conv1 = ConvBNAct(in_channels,hidden_channels,stride=1,padding=0,kernel_size=1)\n        self.conv2 = ConvBNAct(hidden_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.residual = shortcut and (in_channels == out_channels)\n\n    def forward(self,x):\n        out = self.conv2 (self.conv1(x))\n        if (self.residual == True):\n            out = out + x\n        return out\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:56:07.891614Z","iopub.execute_input":"2025-07-20T10:56:07.891976Z","iopub.status.idle":"2025-07-20T10:56:07.898046Z","shell.execute_reply.started":"2025-07-20T10:56:07.891949Z","shell.execute_reply":"2025-07-20T10:56:07.897168Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# C2f Block\n\nC2f = \"Concatenate --> Convolution --> f-block(like Bottleneck)\"","metadata":{}},{"cell_type":"code","source":"class c2f(nn.Module):\n    def __init__(self, in_channels, out_channels,n,d,expansion=0.5,shortcut=True):\n        super().__init__()\n        hidden_channels = int(out_channels * expansion)\n        num_bnecks = max(round(n*d),1)\n        #1. applying 1st conv layer from ConvBNAct\n        self.conv1=ConvBNAct(in_channels,out_channels,kernel_size=1,stride=1,padding=0)\n        #2.splitting this output in 2 parts:\n        # one for passing into bottlenecks and other as it is (for skip connections behaviour)\n        self.layers = nn.ModuleList([\n            Bottleneck(hidden_channels,hidden_channels) for i in range(num_bnecks)\n        ])\n        self.conv2=ConvBNAct((num_bnecks +2)*hidden_channels,out_channels,kernel_size=1,stride=1,padding=0)\n\n    def forward(self,x):\n         x=self.conv1(x)\n         #splitting this obtained image into 2 parts channel wise\n         x1 = x[:,:x.shape[1]//2,:,:] #shape: B,64,320,320\n         x2 = x[:,x.shape[1]//2:,:,:] #shape: B,64,320,320\n\n         outputs = [x1,x2]\n         for layer in self.layers:\n             x1=layer(x1)\n             outputs.append(x1)\n\n         out_final = torch.concat(outputs,dim=1)\n\n         out=self.conv2(out_final)\n         return out\n         ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:56:29.778990Z","iopub.execute_input":"2025-07-20T10:56:29.779350Z","iopub.status.idle":"2025-07-20T10:56:29.786823Z","shell.execute_reply.started":"2025-07-20T10:56:29.779323Z","shell.execute_reply":"2025-07-20T10:56:29.785934Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Spatial Pyramid Pooling Fast (SPPF) Block\n\nTake features from different scales and combine them,so that the network becomes better at capturing both local and global features.","metadata":{}},{"cell_type":"code","source":"class sppf(nn.Module):\n    def __init__(self,in_channels,out_channels,expansion=0.5,num_pool=3):\n        super()._init__()\n        hidden_channels=int(out_channels*expansion)\n        self.conv1 = ConvBNAct(in_channels,hidden_channels,kernel_size=1,stride=1,padding=0)\n    \n        self.poolLayer = nn.ModuleList([\n            nn.MaxPool2d(kernel_size=5,stride=1,padding=2) for i in range(num_pool)\n        ])\n    \n        self.conv2 = ConvBNAct((num_pool + 1)*hidden_channels,out_channels,stride=1,kernel_size=1,padding=0)\n\n    def forward(self,x):\n        x=self.conv1(x)\n        outs = [x]\n\n        for layer in self.poolLayer:\n            y1 = layer(x)\n            outs.append(y1)\n\n        x_cat=torch.concat(outs,dim=1)\n        out=self.conv2(x_cat)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:57:36.241842Z","iopub.execute_input":"2025-07-20T10:57:36.242188Z","iopub.status.idle":"2025-07-20T10:57:36.249148Z","shell.execute_reply.started":"2025-07-20T10:57:36.242164Z","shell.execute_reply":"2025-07-20T10:57:36.248207Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Putting it Together - The Backbone\n\nThe backbone is the feature extractor of the model - it takes in the input image and extracts feature maps at multiple scales (from low-level to high-level). These features are then passed to the neck and head for detection.\n\n","metadata":{}},{"cell_type":"code","source":"class Backbone(nn.Module):\n    def __init__(self,depth_mul=1,width_mul=1):\n        super().__init__()\n\n        def ch(c):\n            return max(int(c*float(width_mul)),1)\n\n        def d(n):\n            return max(round(n*float(depth_mul)),1)\n\n        self.stem = ConvBNAct(3,ch(64),3,2,1)\n\n        self.conv1 = ConvBNAct(ch(64),ch(128),3,2,1)\n        self.stage1=c2f(ch(128),ch(128),n=d(3),d=1)\n\n        self.conv2 = ConvBNAct(ch(128), ch(256), 3, 2, 1)\n        self.stage2 = c2f(ch(256), ch(256), n=d(6), d=1)\n\n        \n        self.conv3 = ConvBNAct(ch(256), ch(512), 3, 2, 1)\n        self.stage3 = c2f(ch(512), ch(512), n=d(6), d=1)\n\n        \n        self.conv4 = ConvBNAct(ch(512), ch(512), 3, 2, 1)\n        self.stage4 = c2f(ch(512), ch(512), n=d(3), d=1)\n\n        self.sppf = sppf(in_channels=ch(512),out_channels=ch(512),num_pool=3)\n\n        self.FinalLayer=nn.Sequential( self.stem,self.conv1, self.stage1,self.conv2, self.stage2,self.conv3, self.stage3,self.conv4, self.stage4,\n            self.sppf\n        )\n\n        self.x1_layer=nn.Sequential(\n            self.stem,self.conv1,self.stage1,self.conv2,self.stage2\n        )\n\n        self.x2_layer = nn.Sequential(\n            self.stem,self.conv1, self.stage1,self.conv2, self.stage2,self.conv3, self.stage3\n        )\n        self.x3_layer = nn.Sequential(\n            self.stem,self.conv1, self.stage1,self.conv2, self.stage2,self.conv3, self.stage3,self.conv4, self.stage4,\n            self.sppf\n        )\n    def forward(self,x):\n        out=self.FinalLayer(x)\n        x1=self.x1_layer(x)\n        x2=self.x2_layer(x)\n        x3=self.x3_layer(x)\n        return out,x1,x2,x3\n\n     \n        \n\n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:57:49.292586Z","iopub.execute_input":"2025-07-20T10:57:49.292937Z","iopub.status.idle":"2025-07-20T10:57:49.303645Z","shell.execute_reply.started":"2025-07-20T10:57:49.292908Z","shell.execute_reply":"2025-07-20T10:57:49.302929Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# NECK Architecture","metadata":{}},{"cell_type":"code","source":"class upsample(nn.Module):\n    def __init__(self,scale_factor = 2, mode=\"nearest\"):\n        super().__init__()\n        self.Upsample = nn.Upsample(scale_factor=scale_factor,mode=mode)\n\n    def forward(self,x):\n        return self.Upsample(x)\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:57:55.336687Z","iopub.execute_input":"2025-07-20T10:57:55.337030Z","iopub.status.idle":"2025-07-20T10:57:55.342028Z","shell.execute_reply.started":"2025-07-20T10:57:55.337008Z","shell.execute_reply":"2025-07-20T10:57:55.341119Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Neck(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.upsample=upsample(scale_factor=2,mode=\"nearest\")\n        self.c2f12 = c2f(in_channels =1024,out_channels=1024,n=3,d=1,shortcut=False)\n        self.c2f15 = c2f(in_channels = 768, out_channels = 768, n = 3, d = 1, shortcut = False)\n        self.down1 = ConvBNAct(in_channels = 768, out_channels = 256, kernel_size = 1, stride = 1, padding = 0)\n        self.conv16 = ConvBNAct(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 2, padding = 1)\n        self.c2f18 = c2f(in_channels = 768, out_channels = 768, n = 3, d = 1, shortcut = False)\n        self.down2 = ConvBNAct(in_channels = 768, out_channels = 512, kernel_size = 1, stride = 1, padding = 0)\n        self.conv19 = ConvBNAct(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 2, padding = 1)\n        self.c2f21 = c2f(in_channels = 1024, out_channels = 1024, n = 3, d = 1, shortcut = False)\n        self.down3 = ConvBNAct(in_channels = 1024, out_channels = 512, kernel_size = 1, stride = 1, padding = 0)\n        self.down4 = ConvBNAct(in_channels = 1024, out_channels = 512, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x1,x2,x3):\n        \n        x_10=self.upsample(x3)\n        x_11=torch.concat([x_10,x_2],dim=1)\n        x_12=self.down4(self.c2f12(x_11))\n        \n        x_13=self.upsample(x_12)\n        x_14=torch.concat([x_13,x1],dim=1)\n        x_15=self.c2f15(x_14)\n        detect_1=self.down1(x_15)\n\n        x_16=self.conv16(detect_1)\n        x_17=torch.concat([x_12,x_16],dim=1)\n        x_18=self.c2f18(x_17)\n        detect_2=self.down2(x_18)\n        \n        x_19=self.conv19(detect_2)\n        x_20=torch.concat([x3,x_19],dim=1)\n        x_21=self.c2f21(x_20)\n        detect_3=self.down3(x_21)\n        \n        return detect_1,detect_2,detect_3\n\n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:01.447274Z","iopub.execute_input":"2025-07-20T10:58:01.447567Z","iopub.status.idle":"2025-07-20T10:58:01.457369Z","shell.execute_reply.started":"2025-07-20T10:58:01.447542Z","shell.execute_reply":"2025-07-20T10:58:01.456477Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# The Detect Box","metadata":{}},{"cell_type":"code","source":"class Detect(nn.Module):\n    def __init__(self,in_channels,num_classes=1,reg_max=15):\n        super().__init__()\n        self.reg_max=reg_max\n        self.num_classes=num_classes\n\n        self.bbox_layer=nn.Sequential(\n            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels,4*(reg_max+1),kernel_size=1,padding=0),\n        )\n\n        self.class_layer = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels, num_classes, kernel_size = 1, padding = 0)\n        )\n\n    def forward(self,x):\n        bounding_box=self.bbox_layer(x)\n        classif_box=self.class_layer(x)\n        return bounding_box,classif_box\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:05.930489Z","iopub.execute_input":"2025-07-20T10:58:05.930836Z","iopub.status.idle":"2025-07-20T10:58:05.937801Z","shell.execute_reply.started":"2025-07-20T10:58:05.930810Z","shell.execute_reply":"2025-07-20T10:58:05.936910Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self,in_channels_list,num_classes=1,reg_max=15):\n        super().__init__()\n        self.head_layer = nn.ModuleList([\n            Detect(in_channels,num_classes,reg_max) for in_channels in in_channels_list\n        ])\n\n    def forward(self,features):\n         all_preds=[]\n         for i,feat in enumerate(features):\n             bbox_pred,cls_pred = self.head_layer[i](feat)\n             all_preds.append((bbox_pred,cls_pred))\n         return all_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:11.290899Z","iopub.execute_input":"2025-07-20T10:58:11.291256Z","iopub.status.idle":"2025-07-20T10:58:11.298994Z","shell.execute_reply.started":"2025-07-20T10:58:11.291231Z","shell.execute_reply":"2025-07-20T10:58:11.298059Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# The YOLOv8 Architecture","metadata":{}},{"cell_type":"code","source":"class YOLOv8(nn.Module):\n    def __init__(self,backbone,neck,head):\n        super(YOLOv8,self).__init__()\n        self.backbone=backbone\n        self.neck=neck\n        self.head=head\n\n    def forward(self,x):\n        _,x1,x2,x3 = self.backbone(x)\n        d1,d2,d3=self.neck(x1,x2,x3)\n        preds=self.head([d1,d2,d3])\n        return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:17.201388Z","iopub.execute_input":"2025-07-20T10:58:17.201652Z","iopub.status.idle":"2025-07-20T10:58:17.207275Z","shell.execute_reply.started":"2025-07-20T10:58:17.201635Z","shell.execute_reply":"2025-07-20T10:58:17.206192Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Helper Functions ","metadata":{}},{"cell_type":"code","source":"def decode_bboxes(bbox_pred, stride, reg_max=15):\n    # Get the shape: B=batch size, _=channels, H=height, W=width\n    B, _, H, W = bbox_pred.shape\n\n    # Reshape from [B, 4*(reg_max+1), H, W] → [B, 4, reg_max+1, H, W]\n    # 4 corresponds to the 4 sides: [left, top, right, bottom]\n    bbox_pred = bbox_pred.view(B, 4, reg_max + 1, H, W)\n\n    # Apply softmax along the bin dimension (dim=2) to get probability distributions\n    prob = F.softmax(bbox_pred, dim=2)\n\n    # Create the bin index tensor: [0, 1, ..., reg_max]\n    proj = torch.arange(reg_max + 1, dtype=prob.dtype, device=prob.device)\n\n    # Compute the expected distance by taking the weighted sum of bin indices\n    dist = (prob * proj[None, None, :, None, None]).sum(dim=2)  # shape: [B, 4, H, W]\n\n    # Generate a grid of (x, y) locations corresponding to feature map positions\n    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n    grid = torch.stack((grid_x, grid_y), dim=0).to(bbox_pred.device)  # shape: [2, H, W]\n\n    # Compute center coordinates in input image space\n    x_center = (grid[0][None] + 0.5) * stride  # shape: [1, H, W]\n    y_center = (grid[1][None] + 0.5) * stride  # shape: [1, H, W]\n\n    # Convert predicted distances (dist) to bounding box corners\n    x1 = x_center - dist[:, 0] * stride  # left\n    y1 = y_center - dist[:, 1] * stride  # top\n    x2 = x_center + dist[:, 2] * stride  # right\n    y2 = y_center + dist[:, 3] * stride  # bottom\n\n    # Convert corner format [x1, y1, x2, y2] to center format [xc, yc, w, h]\n    xc = (x1 + x2) / 2\n    yc = (y1 + y2) / 2\n    w = x2 - x1\n    h = y2 - y1\n\n    # Stack to final bounding box shape: [B, H*W, 4] in (xc, yc, w, h) format\n    bboxes = torch.stack([xc, yc, w, h], dim=-1)\n    return bboxes.view(B, -1, 4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:25.363203Z","iopub.execute_input":"2025-07-20T10:58:25.363456Z","iopub.status.idle":"2025-07-20T10:58:25.371295Z","shell.execute_reply.started":"2025-07-20T10:58:25.363439Z","shell.execute_reply":"2025-07-20T10:58:25.370391Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"This function processes class prediction scores by applying a sigmoid activation to convert raw logits into probabilities. It then flattens the predictions and creates a binary mask (`score_mask`) that marks positions where the confidence score exceeds a given threshold (`score_thresh`).\n","metadata":{}},{"cell_type":"code","source":"def process_cls_scores(cls_pred,score_thresh=0.4):\n    B,C,H,W = cls_pred.shape\n    assert C == 1\n    probs=torch.sigmoid(cls_pred)\n    probs=probs.view(B,-1)\n    score_mask = probs > score_thresh\n    return score_mask,probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:29.131104Z","iopub.execute_input":"2025-07-20T10:58:29.131760Z","iopub.status.idle":"2025-07-20T10:58:29.136114Z","shell.execute_reply.started":"2025-07-20T10:58:29.131730Z","shell.execute_reply":"2025-07-20T10:58:29.135297Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"This function filters and formats model predictions (bounding boxes and classification scores) into a usable list of detections with coordinates and confidence scores, suitable for evaluation or visualization. It combines multi-scale predictions (from different strides) and selects only high-confidence boxes.","metadata":{}},{"cell_type":"code","source":"def get_final_predictions(preds, strides=[8, 16, 32], score_thresh=0.4):\n    \n    all_boxes = []  # To collect final bounding boxes with scores\n    \n    for i, (bbox_pred, cls_pred) in enumerate(preds):\n        stride = strides[i]  # Each feature map corresponds to a stride (scale)\n        \n        # Decode the bounding boxes from distribution format to [x, y, w, h]\n        boxes = decode_bboxes(bbox_pred, stride=stride)\n        \n        # Process classification scores and get a mask of confident predictions\n        conf_mask, confs = process_cls_scores(cls_pred, score_thresh=score_thresh)\n\n        # Iterate through each sample in the batch\n        for batch_idx in range(boxes.shape[0]):\n            batch_boxes = boxes[batch_idx]  # Predicted boxes for one image [H*W, 4]\n            batch_mask = conf_mask[batch_idx]  # Boolean mask for scores > threshold\n            batch_confs = confs[batch_idx]     # Confidence values [H*W]\n\n            # Select only the valid boxes based on confidence threshold\n            valid_boxes = batch_boxes[batch_mask]\n            valid_scores = batch_confs[batch_mask]\n\n            # Collect the filtered boxes and scores\n            for j in range(valid_boxes.size(0)):\n                x, y, w, h = valid_boxes[j]     # Bounding box in [xc, yc, w, h]\n                score = valid_scores[j].item()  # Corresponding confidence score\n                all_boxes.append((x.item(), y.item(), w.item(), h.item(), score))  # Add to results\n\n    return all_boxes  # List of all final predicted boxes with scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:38.317400Z","iopub.execute_input":"2025-07-20T10:58:38.318013Z","iopub.status.idle":"2025-07-20T10:58:38.324764Z","shell.execute_reply.started":"2025-07-20T10:58:38.317985Z","shell.execute_reply":"2025-07-20T10:58:38.323786Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def decode_from_distr(pred, reg_max=15):\n    \n    B, channels, H, W = pred.shape\n    pred = pred.view(B, 4, reg_max + 1, H, W)\n    \n    # Apply softmax\n    prob = F.softmax(pred, dim=2)\n    \n    # Create projection weights\n    proj = torch.arange(reg_max + 1, dtype=prob.dtype, device=prob.device)\n    \n    # Calculate expected values\n    exp = torch.sum(prob * proj[None, None, :, None, None], dim=2)\n    \n    # Generate grid coordinates\n    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n    grid = torch.stack((grid_x, grid_y), dim=0).to(pred.device)\n    \n    # Decode to xyxy format (simplified version)\n    decoded = exp.view(B, 4, -1).permute(0, 2, 1)  # [B, H*W, 4]\n    return decoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:43.529225Z","iopub.execute_input":"2025-07-20T10:58:43.529820Z","iopub.status.idle":"2025-07-20T10:58:43.536054Z","shell.execute_reply.started":"2025-07-20T10:58:43.529792Z","shell.execute_reply":"2025-07-20T10:58:43.534940Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"The `get_dfl_targets` function initializes a zero tensor to hold **distributional regression targets** for bounding box distances in a format compatible with **DFL (Distribution Focal Loss)**. Each box gets a `[4, reg_max + 1]` target representing class-like labels over bins for the four box directions (left, top, right, bottom).\n","metadata":{}},{"cell_type":"code","source":"def get_dfl_targets(boxes, reg_max=15):\n    \n    B, N, _ = boxes.shape\n    targets = torch.zeros(B, N, 4, reg_max + 1, device=boxes.device)\n    return targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:48.081581Z","iopub.execute_input":"2025-07-20T10:58:48.082381Z","iopub.status.idle":"2025-07-20T10:58:48.087594Z","shell.execute_reply.started":"2025-07-20T10:58:48.082343Z","shell.execute_reply":"2025-07-20T10:58:48.086574Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def make_anchors(feats, strides):\n    \"\"\"Generate anchor points.\"\"\"\n    anchor_points, stride_tensor = [], []\n\n    for i, stride in enumerate(strides):\n        h, w = feats[i].shape[2:]\n        y, x = torch.arange(h, dtype=torch.float32), torch.arange(w, dtype=torch.float32)\n        yv, xv = torch.meshgrid(y, x, indexing='ij')\n\n        grid = torch.stack([xv, yv], 2).view(-1, 2)\n        anchor_points.append(grid * stride + stride / 2)\n        stride_tensor.append(torch.full((grid.shape[0], 1), stride, dtype=torch.float32))\n\n    anchor_points = torch.cat(anchor_points, 0).to(feats[0].device)\n    stride_tensor = torch.cat(stride_tensor, 0).to(feats[0].device)\n\n    return anchor_points, stride_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:59:32.652049Z","iopub.execute_input":"2025-07-20T10:59:32.652422Z","iopub.status.idle":"2025-07-20T10:59:32.659035Z","shell.execute_reply.started":"2025-07-20T10:59:32.652400Z","shell.execute_reply":"2025-07-20T10:59:32.658201Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def decode_bboxes_dfl(bbox_pred, anchor_points, stride_tensor, reg_max=15):\n    \"\"\"Decode bbox predictions from DFL format.\"\"\"\n    batch_size, num_anchors, _ = bbox_pred.shape\n    bbox_pred = bbox_pred.view(batch_size, num_anchors, 4, reg_max + 1)\n    bbox_pred = F.softmax(bbox_pred, dim=-1)\n\n    proj = torch.arange(reg_max + 1, dtype=bbox_pred.dtype, device=bbox_pred.device)\n    bbox_pred = (bbox_pred * proj[None, None, None, :]).sum(dim=-1)\n\n    lt, rb = bbox_pred.chunk(2, dim=-1)\n    anchor_points = anchor_points[None].expand(batch_size, -1, -1)\n    stride_tensor = stride_tensor[None].expand(batch_size, -1, -1)\n\n    x1y1 = anchor_points - lt * stride_tensor\n    x2y2 = anchor_points + rb * stride_tensor\n\n    return torch.cat([x1y1, x2y2], dim=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:59:36.090596Z","iopub.execute_input":"2025-07-20T10:59:36.090932Z","iopub.status.idle":"2025-07-20T10:59:36.097363Z","shell.execute_reply.started":"2025-07-20T10:59:36.090908Z","shell.execute_reply":"2025-07-20T10:59:36.096479Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Non - Max Suppression","metadata":{}},{"cell_type":"code","source":"import torchvision.ops as ops\n\ndef apply_nms(boxes,scores,iou_threshold=0.5):\n    if boxes.numel()==0:\n        return torch.tensor([],dtype=torch.int64)\n    # Convert [xc, yc, w, h] → [x1, y1, x2, y2]\n    x1 = boxes[:, 0] - boxes[:, 2] / 2\n    y1 = boxes[:, 1] - boxes[:, 3] / 2\n    x2 = boxes[:, 0] + boxes[:, 2] / 2\n    y2 = boxes[:, 1] + boxes[:, 3] / 2\n    boxes_xyxy=torch.stack(([x1,y1,x2,y2]),dim=1)\n    keep=ops.nms(boxes_xyxy,scores,iou_threshold=iou_threshold)\n    return keep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:58:55.329079Z","iopub.execute_input":"2025-07-20T10:58:55.329374Z","iopub.status.idle":"2025-07-20T10:58:59.729296Z","shell.execute_reply.started":"2025-07-20T10:58:55.329350Z","shell.execute_reply":"2025-07-20T10:58:59.728319Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Defining Loss Functions","metadata":{}},{"cell_type":"markdown","source":"### 📊 Comparison of YOLOv8 Loss Functions\n\n| Loss Function               | Used For              | Formula                                                                                                       | Key Characteristics                                                                                                                                                             |\n|----------------------------|------------------------|---------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Focal Loss**             | Classification         | $\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)$<br>where $p_t = \\begin{cases} p & y = 1 \\\\ 1 - p & y = 0 \\end{cases}$ | - Tackles class imbalance by focusing training on hard negatives<br>- Uses parameters $\\gamma$ (focusing factor), $\\alpha$ (class balance)<br>- Common in dense detectors like RetinaNet and YOLO |\n| **Distribution Focal Loss** (DFL) | Box Side Regression     | $\\text{DFL} = (1 - w) \\cdot \\text{NLL}(l) + w \\cdot \\text{NLL}(r)$<br>where $t = l + w$, $l = \\lfloor t \\rfloor$, $r = \\lceil t \\rceil$ | - Treats bounding box regression as classification over discrete bins<br>- Improves localization accuracy<br>- Used for bounding box side predictions in YOLOv8                 |\n| **CIoU Loss**              | Bounding Box Regression | $\\text{CIoU} = 1 - \\text{IoU} + \\frac{\\rho^2(\\mathbf{b}, \\mathbf{b}^{gt})}{c^2}$<br>where $\\rho$ is center distance, $c$ is diagonal of smallest enclosing box | - Improves over IoU by considering object center distance and aspect ratio<br>- Encourages better convergence<br>- Strong performance in precise localization tasks             |\n","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self,gamma=2.0,alpha=0.25,reduction='mean'):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n\n     def forward(self,logits,targets):\n         probs=torch.sigmoid(logits)\n         ce_loss=F.binary_cross_entropy_with_logits(logits,targets,reduction='none')\n         p_t=targets*probs+(1-targets)*(1-probs)\n         loss=ce_loss*((1-p_t)**self.gamma)\n\n         if self.alpha >= 0:\n             alpha_t = self.alpha 8*targets +(1-self.alpha)*(1-targets)\n             loss *= alpha_t\n\n         if self.reduction == 'mean' :\n             return loss.mean()\n         elif self.reduction == 'sum':\n             return loss.sum()\n         return loss\n         ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DistributionFocalLoss(nn.Module):\n    def __init__(self, reg_max=15):\n        super().__init__()\n        self.reg_max = reg_max\n\n    def forward(self, pred, target):\n        # FIXED: Added clamping to prevent index overflow\n        dis_left = target.long()\n        dis_right = torch.clamp(dis_left + 1, max=self.reg_max)\n\n        weight_right = target - dis_left\n        weight_left = 1 - weight_right\n\n        pred = pred.log_softmax(dim=-1)\n        loss_left = F.nll_loss(pred.view(-1, pred.shape[-1]), dis_left.view(-1), reduction='none')\n        loss_right = F.nll_loss(pred.view(-1, pred.shape[-1]), dis_right.view(-1), reduction='none')\n\n        loss = (weight_left.view(-1) * loss_left + weight_right.view(-1) * loss_right).view(target.shape)\n        return loss.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bbox_ciou_loss(pred_boxes, target_boxes, eps=1e-7):\n    \"\"\"\n    pred_boxes, target_boxes: [N, 4] in xyxy format\n    \"\"\"\n    x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n    y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n    x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n    y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n\n    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n    area_pred = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n    area_target = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n    union = area_pred + area_target - inter + eps\n    iou = inter / union\n\n    # Center distance\n    px = (pred_boxes[:, 0] + pred_boxes[:, 2]) / 2\n    py = (pred_boxes[:, 1] + pred_boxes[:, 3]) / 2\n    gx = (target_boxes[:, 0] + target_boxes[:, 2]) / 2\n    gy = (target_boxes[:, 1] + target_boxes[:, 3]) / 2\n    center_dist = (px - gx) ** 2 + (py - gy) ** 2\n\n    # Enclosing box diagonal\n    x1_c = torch.min(pred_boxes[:, 0], target_boxes[:, 0])\n    y1_c = torch.min(pred_boxes[:, 1], target_boxes[:, 1])\n    x2_c = torch.max(pred_boxes[:, 2], target_boxes[:, 2])\n    y2_c = torch.max(pred_boxes[:, 3], target_boxes[:, 3])\n    diagonal = ((x2_c - x1_c) ** 2 + (y2_c - y1_c) ** 2) + eps\n\n    ciou = iou - (center_dist / diagonal)\n    loss = 1.0 - ciou.clamp(min=-1.0, max=1.0)\n    return loss.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}