{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Modules"
      ],
      "metadata": {
        "id": "9fueq5bWUhLP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPNHCj8uShbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Patch Embeddings"
      ],
      "metadata": {
        "id": "Z2jcxQnlVSYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,d_model,img_size,patch_size,n_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model=d_model #Dimensionality of Model(size of the embedding vector that representd each patch)\n",
        "    self.img_size = img_size # Image Size\n",
        "    self.patch_size = patch_size # Patch Size\n",
        "    self.n_channels = n_channels # Number of Channels\n",
        "\n",
        "    self.linear_project=nn.Conv2d(self.n_channels,self.d_model,kernel_size=self.patch_size,stride=self.patch_size)\n",
        "\n",
        "  # B: Batch Size\n",
        "  # C: Image Channels\n",
        "  # H: Image Height\n",
        "  # W: Image Width\n",
        "  # P_col: Patch Column\n",
        "  # P_row: Patch Row\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "      x=self.linear_project(x) # (B,C,H,W) --> (B,d_model,P_col,P_row)\n",
        "      x=x.flatten(2)  # (B,d_model,P_col,p_row) --> (B,d_model,P)\n",
        "      x=x.transpose(1,2) # (B,d_model,P) --> (B,P,d_model)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Aabnp2MVa5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Token and Positional Encoding"
      ],
      "metadata": {
        "id": "5d0--cI0Z1dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,max_seq_length):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cls_token=nn.Parameter(torch.randn(1,1,d_model))  # Classification Token\n",
        "\n",
        "    # Creating positional encoding\n",
        "    pe=torch.zeros(max_seq_length,d_model)\n",
        "\n",
        "    for pos in range(max_seq_length):\n",
        "      for i in range(d_model):\n",
        "        if i%2 ==0:\n",
        "          pe[pos][i]=np.sin(pos/(10000**(i/d_model)))\n",
        "        else:\n",
        "          pe[pos][i]=np.cos(pos/(10000**((i-1)/d_model)))\n",
        "\n",
        "    self.register_buffer('pe',pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # Expand to have class token for every image in batch\n",
        "    tokens_batch=self.cls_token.expand(x.size()[0],-1,-1)  # (1,1,d_model) --> (batch_size,1,d_model)\n",
        "    # Adding class tokens to the beginning of each embedding\n",
        "    x=torch.cat((tokens_batch,x),dim=1)  # (batch_size,num_patches,d_model) --> (batch_size,1+num_patches,d_model)\n",
        "    # Add positional encoding to embeddings\n",
        "    x=x+self.pe\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "xvQSzzg_a5kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Head"
      ],
      "metadata": {
        "id": "2NFojIUkrzIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self,d_model,head_size):\n",
        "    super().__init__()\n",
        "    self.head_size=head_size\n",
        "\n",
        "    self.query = nn.Linear(d_model, head_size)\n",
        "    self.key = nn.Linear(d_model, head_size)\n",
        "    self.value = nn.Linear(d_model, head_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # Obtaining Queries, Keys and Values\n",
        "    Q=self.query(x)  # (batch_size,num_heads,seq_length,head_size)\n",
        "    K=self.key(x)\n",
        "    V=self.value(x)\n",
        "\n",
        "    # Dot product of Queries and Keys\n",
        "    attention= Q @ K.transpose(-2,-1)  #(batch_size,num_heads,seq_length,seq_length)\n",
        "\n",
        "    # Scaling\n",
        "    attention = attention / (self.head_size ** 0.5)\n",
        "    attention=torch.softmax(attention,dim=-1)\n",
        "    attention=attention @ V\n",
        "\n",
        "    return attention\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4gAsveCUr2G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Head Attention"
      ],
      "metadata": {
        "id": "mqgg0f2Pz002"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,n_heads):\n",
        "    super().__init__()\n",
        "    self.head_size=d_model//n_heads\n",
        "\n",
        "    self.W_0=nn.Linear(d_model,d_model)\n",
        "    self.heads=nn.ModuleList([AttentionHead(d_model,self.head_size) for _ in range(n_heads)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    #Combine attention heads\n",
        "\n",
        "    out=torch.cat([head(x) for head in self.heads],dim=-1)\n",
        "    out=self.W_0(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "xb908jVbz4jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder"
      ],
      "metadata": {
        "id": "00K6OIX93ycH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,r_mlp=4):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.n_heads=n_heads\n",
        "\n",
        "    # Sub-Layer 1 Normalization\n",
        "    self.ln1=nn.LayerNorm(d_model)\n",
        "\n",
        "    # Multi-Head Attention\n",
        "    self.mha=MultiHeadAttention(d_model,n_heads)\n",
        "\n",
        "    # Sub-Layer 2 Normalization\n",
        "    self.ln2=nn.LayerNorm(d_model)\n",
        "\n",
        "    # MultiLayer Perception\n",
        "    self.mlp=nn.Sequential(\n",
        "        nn.Linear(d_model,d_model*r_mlp),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*r_mlp,d_model)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    # Residual Connection After Sub-Layer 1\n",
        "    out = x + self.mha(self.ln1(x))\n",
        "\n",
        "    # Residual Connection After Sub-Layer 2\n",
        "    out = out + self.mlp(self.ln2(out))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WITCEuOd318f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer"
      ],
      "metadata": {
        "id": "JXbUQCMZB5Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,d_model,n_classes,img_size,patch_size,m_channels,n_heads,n_layers):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "    self.d_model = d_model # Dimensionality of model\n",
        "    self.n_classes = n_classes # Number of classes\n",
        "    self.img_size = img_size # Image size\n",
        "    self.patch_size = patch_size # Patch size\n",
        "    self.n_channels = n_channels # Number of channels\n",
        "    self.n_heads = n_heads # Number of attention heads\n",
        "\n",
        "    self.n_patches = (self.img_size[0]*self.img_size[1]) // (self.patch_size[0]*self.patch_size[1])\n",
        "    self.max_seq_length=self.n_patches+1\n",
        "\n",
        "    self.patch_embedding=PatchEmbedding(self.d_model,self.img_size,self.patch_size,self.n_channels)\n",
        "    self.positional_encoding=PositionalEncoding(self.d_model,self.max_seq_length)\n",
        "    self.transformer_encoder=nn.Sequential(*[TransformerEncoder(self.d_model,self.n_heads) for _ in range(n_layers)])\n",
        "\n",
        "    # Classification MLP\n",
        "\n",
        "    self.classifier=nn.Sequential(\n",
        "        nn.Linear(self.d_model,self.n_classes),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,images):\n",
        "\n",
        "    x=self.patch_embedding(images)\n",
        "    x=self.positional_encoding(x)\n",
        "    x=self.transformer_encoder(x)\n",
        "    x=self.classifier(x[:,0])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Up0ZRdF1B_vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters"
      ],
      "metadata": {
        "id": "iXsgiHnEILAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model=9\n",
        "n_classes=10\n",
        "img_size=(32,32)\n",
        "patch_size=(16,16)\n",
        "n_channels=1\n",
        "n_heads=3\n",
        "n_layers=3\n",
        "batch_size=128\n",
        "epochs=5\n",
        "alpha=0.005"
      ],
      "metadata": {
        "id": "TBdKWw2OIPI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the MNIST Dataset"
      ],
      "metadata": {
        "id": "KszjrQ9uIkOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=T.Compose([T.Resize(img_size),T.ToTensor()])\n",
        "\n",
        "train_set=MNIST(root=\"./../datasets\", train=True, download=True, transform=transform)\n",
        "test_set = MNIST(root=\"./../datasets\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader=DataLoader(train_set,shuffle=True,batch_size=batch_size)\n",
        "test_loader=DataLoader(test_set,shuffle=False,batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "qq9dwLz1IorV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "tNnrhoDXMai4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \",device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "\n",
        "transformer=VisionTransformer(d_model,n_classes,img_size,patch_size,n_channels,n_heads,n_layers).to(device)\n",
        "\n",
        "optimizer=Adam(transformer.parameters(),lr=alpha)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  training_loss=0.0\n",
        "  for i,data in enumerate(train_loader,0):\n",
        "    inputs,labels=data\n",
        "    inputs,labels=inputs.to(device),labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs=transformer(inputs)\n",
        "    loss=criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss+=loss.item()\n",
        "\n",
        "  print(f'Epoch{epoch+1}/{epochs} loss:{training_loss / len(train_loader) :.3f} ')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KfdfxIYXMZ4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e53c06-fec3-4ccf-bf10-39877e75d99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cpu \n",
            "Epoch1/5 loss:1.838 \n",
            "Epoch2/5 loss:1.677 \n",
            "Epoch3/5 loss:1.564 \n",
            "Epoch4/5 loss:1.550 \n",
            "Epoch5/5 loss:1.544 \n"
          ]
        }
      ]
    }
  ]
}