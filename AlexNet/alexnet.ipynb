{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing the Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:31:50.096858Z","iopub.execute_input":"2025-08-13T16:31:50.097446Z","iopub.status.idle":"2025-08-13T16:31:57.342908Z","shell.execute_reply.started":"2025-08-13T16:31:50.097424Z","shell.execute_reply":"2025-08-13T16:31:57.342311Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Loading the Dataset","metadata":{}},{"cell_type":"code","source":"def get_train_valid_loader(data_dir,batch_size,augment,random_seed,valid_size=0.1,shuffle=True):\n    normalize=transforms.Normalize(\n        mean=[0.4914, 0.4822, 0.4465],\n        std=[0.2023, 0.1994, 0.2010],\n        \n    )\n\n    # define transforms\n    valid_transform=transforms.Compose([\n        transforms.Resize((227,227)),\n        transforms.ToTensor(),\n        normalize,\n        \n    ])\n    if augment:\n        train_transform=transforms.Compose([\n            transforms.RandomCrop(32,padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ])\n    else:train_transform = transforms.Compose([\n                transforms.Resize((227,227)),\n                transforms.ToTensor(),\n                normalize,\n            ])\n\n    # load the dataset\n    train_dataset=datasets.CIFAR10(\n        root=data_dir,train=True,\n        download=True,transform=train_transform,\n    )\n    \n    valid_dataset = datasets.CIFAR10(\n            root=data_dir, train=True,\n            download=True, transform=valid_transform,\n        )\n    num_train=len(train_dataset)\n    indices=list(range(num_train))\n    split=int(np.floor(valid_size*num_train))\n\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_idx,valid_idx=indices[split:],indices[:split]\n    train_sampler=SubsetRandomSampler(train_idx)\n    valid_sampler=SubsetRandomSampler(valid_idx)\n\n    train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,sampler=train_sampler)\n    valid_loader=torch.utils.data.DataLoader(valid_dataset,batch_size=batch_size,sampler=valid_sampler)\n\n    return(train_loader,valid_loader)\n\n\n\ndef get_test_loader(data_dir,\n                        batch_size,\n                        shuffle=True):\n        normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n\n        # define transform\n        transform = transforms.Compose([\n            transforms.Resize((227,227)),\n            transforms.ToTensor(),\n            normalize,\n        ])\n\n        dataset = datasets.CIFAR10(\n            root=data_dir, train=False,\n            download=True, transform=transform,\n        )\n\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=shuffle\n        )\n\n        return data_loader\n\n\n\n\n\n# CIFAR10 dataset \ntrain_loader, valid_loader = get_train_valid_loader(data_dir = './data',batch_size = 64,augment = False,random_seed = 1)\n\ntest_loader = get_test_loader(data_dir = './data',batch_size = 64)\n\n    \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:45:56.660233Z","iopub.execute_input":"2025-08-13T16:45:56.660865Z","iopub.status.idle":"2025-08-13T16:45:59.059447Z","shell.execute_reply.started":"2025-08-13T16:45:56.660841Z","shell.execute_reply":"2025-08-13T16:45:59.058848Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# AlexNet from scratch","metadata":{}},{"cell_type":"markdown","source":"| Block      | Layer & Params                                         | What the params do                                                                                      | Why AlexNet chose them                                                                                                                                               | Practical effect                                                             |\n| ---------- | ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n| **layer1** | `Conv2d(3, 96, kernel_size=11, stride=4, padding=0)`   | 3→96 channels; big **11×11** receptive field; **stride 4** downsampling; **no padding** shrinks the map | Early, aggressive spatial compression to make training/inference tractable on 2012 GPUs; large kernel to capture coarse edges/color blobs and global context quickly | 227→**55** (per side) after conv; strong low-level features with big context |\n|            | `BatchNorm2d(96)`                                      | Normalizes each of 96 feature maps                                                                      | (Modern replacement for LRN) stabilizes/accelerates training, reduces covariate shift                                                                                | Smoother optimization; allows higher learning rates                          |\n|            | `ReLU()`                                               | Half-wave nonlinearity                                                                                  | Cheap, effective nonlinearity; mitigates vanishing gradients                                                                                                         | Sparsifies activations, improves convergence                                 |\n|            | `MaxPool2d(kernel_size=3, stride=2)`                   | 3×3 max + **stride 2**                                                                                  | Makes the network translation-tolerant and shrinks spatial size while keeping strong responses                                                                       | 55→**27** (per side); keeps strongest local features                         |\n| **layer2** | `Conv2d(96, 256, kernel_size=5, stride=1, padding=2)`  | 96→256 channels; **5×5** receptive field; **same** spatial size due to padding=2                        | Increase channel capacity to model more complex motifs; 5×5 refines patterns discovered by large first conv                                                          | 27→**27** (same size), richer representations                                |\n|            | `BatchNorm2d(256)`                                     | Normalize 256 maps                                                                                      | Stabilize deeper stack                                                                                                                                               | Better gradients                                                             |\n|            | `ReLU()`                                               | Nonlinear mixing                                                                                        | Standard                                                                                                                                                             | –                                                                            |\n|            | `MaxPool2d(3, 2)`                                      | Downsample by \\~2                                                                                       | Control memory/compute; add invariance                                                                                                                               | 27→**13**                                                                    |\n| **layer3** | `Conv2d(256, 384, kernel_size=3, stride=1, padding=1)` | 256→384; **3×3**, **same** size                                                                         | Move to smaller kernels; stacked 3×3s approximate larger receptive fields with fewer params; increases depth and capacity                                            | 13→**13**; more channels for mid/high-level parts                            |\n|            | `BatchNorm2d(384)`                                     | Normalize                                                                                               | As above                                                                                                                                                             | As above                                                                     |\n|            | `ReLU()`                                               | –                                                                                                       | –                                                                                                                                                                    | –                                                                            |\n| **layer4** | `Conv2d(384, 384, kernel_size=3, stride=1, padding=1)` | 384→384; 3×3                                                                                            | Deepens nonlinearity at same resolution to learn complex part co-occurrences                                                                                         | 13→**13**; preserves spatial detail                                          |\n|            | `BatchNorm2d(384)`                                     | Normalize                                                                                               | –                                                                                                                                                                    | –                                                                            |\n|            | `ReLU()`                                               | –                                                                                                       | –                                                                                                                                                                    | –                                                                            |\n| **layer5** | `Conv2d(384, 256, kernel_size=3, stride=1, padding=1)` | 384→256; 3×3                                                                                            | Funnel to fewer channels before final pooling/FC; reduces parameters downstream                                                                                      | 13→**13**                                                                    |\n|            | `BatchNorm2d(256)`                                     | Normalize                                                                                               | –                                                                                                                                                                    | –                                                                            |\n|            | `ReLU()`                                               | –                                                                                                       | –                                                                                                                                                                    | –                                                                            |\n|            | `MaxPool2d(3, 2)`                                      | Final spatial downsample                                                                                | Prepares compact feature grid for FC                                                                                                                                 | 13→**6**                                                                     |\n","metadata":{}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n    def __init__(self,num_classes=10):\n        super(AlexNet,self).__init__()\n        self.layer1=nn.Sequential(\n                nn.Conv2d(3,96,kernel_size=11,stride=4,padding=0),\n                nn.BatchNorm2d(96),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3,stride=2)\n            )\n        self.layer2 = nn.Sequential(\n                nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(256),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size = 3, stride = 2))\n        self.layer3 = nn.Sequential(\n                nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(384),\n                nn.ReLU())\n        self.layer4 = nn.Sequential(\n                nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(384),\n                nn.ReLU())\n        self.layer5 = nn.Sequential(\n                nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size = 3, stride = 2))\n        self.fc = nn.Sequential(\n                nn.Dropout(0.5),\n                nn.Linear(9216, 4096),\n                nn.ReLU())\n        self.fc1 = nn.Sequential(\n                nn.Dropout(0.5),\n                nn.Linear(4096, 4096),\n                nn.ReLU())\n        self.fc2= nn.Sequential(\n                nn.Linear(4096, num_classes))\n    def forward(self, x):\n            out = self.layer1(x)\n            out = self.layer2(out)\n            out = self.layer3(out)\n            out = self.layer4(out)\n            out = self.layer5(out)\n            out = out.reshape(out.size(0), -1)\n            out = self.fc(out)\n            out = self.fc1(out)\n            out = self.fc2(out)\n            return out                          \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:44:20.922199Z","iopub.execute_input":"2025-08-13T16:44:20.922971Z","iopub.status.idle":"2025-08-13T16:44:20.934029Z","shell.execute_reply.started":"2025-08-13T16:44:20.922933Z","shell.execute_reply":"2025-08-13T16:44:20.933208Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Setting Hyperparameters","metadata":{}},{"cell_type":"code","source":"num_classes = 10\nnum_epochs = 20\nbatch_size = 64\nlearning_rate = 0.005\n\nmodel = AlexNet(num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n    \n# Train the model\ntotal_step = len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:46:06.557582Z","iopub.execute_input":"2025-08-13T16:46:06.557816Z","iopub.status.idle":"2025-08-13T16:46:07.105979Z","shell.execute_reply.started":"2025-08-13T16:46:06.557800Z","shell.execute_reply":"2025-08-13T16:46:07.105438Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for i,(images,labels) in enumerate(train_loader):\n        # Move tensors to the configured device\n        images=images.to(device)\n        labels=labels.to(device)\n        \n        # Forward pass\n        outputs=model(images)\n        loss=criterion(outputs,labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}'.format(epoch+1,num_epochs,i+1,total_step,loss.item()))\n    \n\n\n    # Validation\n    with torch.no_grad():\n                correct = 0\n                total = 0\n                for images, labels in valid_loader:\n                    images = images.to(device)\n                    labels = labels.to(device)\n                    outputs = model(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n                    del images, labels, outputs\n    \n                print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))                      \n    \n    \n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:52:25.796505Z","iopub.execute_input":"2025-08-13T16:52:25.796769Z","iopub.status.idle":"2025-08-13T17:19:35.493831Z","shell.execute_reply.started":"2025-08-13T16:52:25.796748Z","shell.execute_reply":"2025-08-13T17:19:35.492997Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\nEpoch [1/20], Step[704/704], Loss: 1.0224\nAccuracy of the network on the 5000 validation images: 76.08 %\nEpoch [2/20], Step[704/704], Loss: 1.3559\nAccuracy of the network on the 5000 validation images: 77.92 %\nEpoch [3/20], Step[704/704], Loss: 0.9896\nAccuracy of the network on the 5000 validation images: 76.58 %\nEpoch [4/20], Step[704/704], Loss: 2.1815\nAccuracy of the network on the 5000 validation images: 76.26 %\nEpoch [5/20], Step[704/704], Loss: 0.4569\nAccuracy of the network on the 5000 validation images: 79.76 %\nEpoch [6/20], Step[704/704], Loss: 1.0018\nAccuracy of the network on the 5000 validation images: 80.64 %\nEpoch [7/20], Step[704/704], Loss: 0.5591\nAccuracy of the network on the 5000 validation images: 80.02 %\nEpoch [8/20], Step[704/704], Loss: 0.9269\nAccuracy of the network on the 5000 validation images: 80.98 %\nEpoch [9/20], Step[704/704], Loss: 0.1442\nAccuracy of the network on the 5000 validation images: 81.26 %\nEpoch [10/20], Step[704/704], Loss: 0.2860\nAccuracy of the network on the 5000 validation images: 79.74 %\nEpoch [11/20], Step[704/704], Loss: 0.2610\nAccuracy of the network on the 5000 validation images: 80.7 %\nEpoch [12/20], Step[704/704], Loss: 0.3947\nAccuracy of the network on the 5000 validation images: 82.4 %\nEpoch [13/20], Step[704/704], Loss: 0.3583\nAccuracy of the network on the 5000 validation images: 82.78 %\nEpoch [14/20], Step[704/704], Loss: 0.6508\nAccuracy of the network on the 5000 validation images: 79.22 %\nEpoch [15/20], Step[704/704], Loss: 0.2283\nAccuracy of the network on the 5000 validation images: 83.2 %\nEpoch [16/20], Step[704/704], Loss: 1.1237\nAccuracy of the network on the 5000 validation images: 81.64 %\nEpoch [17/20], Step[704/704], Loss: 0.6601\nAccuracy of the network on the 5000 validation images: 82.9 %\nEpoch [18/20], Step[704/704], Loss: 0.4789\nAccuracy of the network on the 5000 validation images: 82.96 %\nEpoch [19/20], Step[704/704], Loss: 0.1581\nAccuracy of the network on the 5000 validation images: 83.26 %\nEpoch [20/20], Step[704/704], Loss: 0.3034\nAccuracy of the network on the 5000 validation images: 82.5 %\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            del images, labels, outputs\n\n        print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T17:19:58.633151Z","iopub.execute_input":"2025-08-13T17:19:58.633620Z","iopub.status.idle":"2025-08-13T17:20:16.732405Z","shell.execute_reply.started":"2025-08-13T17:19:58.633596Z","shell.execute_reply":"2025-08-13T17:20:16.731513Z"}},"outputs":[{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 83.07 %\n","output_type":"stream"}],"execution_count":19}]}