{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install labml --upgrade\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T10:28:25.777076Z","iopub.execute_input":"2025-07-06T10:28:25.777510Z","iopub.status.idle":"2025-07-06T10:28:31.848273Z","shell.execute_reply.started":"2025-07-06T10:28:25.777474Z","shell.execute_reply":"2025-07-06T10:28:31.847213Z"}},"outputs":[{"name":"stdout","text":"Collecting labml\n  Downloading labml-0.5.3-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from labml) (3.1.44)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from labml) (6.0.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from labml) (1.26.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->labml) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->labml) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->labml) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->labml) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->labml) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->labml) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->labml) (2.4.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->labml) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->labml) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->labml) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->labml) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->labml) (2024.2.0)\nDownloading labml-0.5.3-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: labml\nSuccessfully installed labml-0.5.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install labml_helpers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T10:31:23.914053Z","iopub.execute_input":"2025-07-06T10:31:23.914924Z","iopub.status.idle":"2025-07-06T10:32:46.875402Z","shell.execute_reply.started":"2025-07-06T10:31:23.914885Z","shell.execute_reply":"2025-07-06T10:32:46.874113Z"}},"outputs":[{"name":"stdout","text":"Collecting labml_helpers\n  Downloading labml_helpers-0.4.89-py3-none-any.whl.metadata (1.4 kB)\nCollecting labml>=0.4.158 (from labml_helpers)\n  Downloading labml-0.5.3-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from labml_helpers) (2.6.0+cu124)\nRequirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from labml>=0.4.158->labml_helpers) (3.1.44)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from labml>=0.4.158->labml_helpers) (6.0.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from labml>=0.4.158->labml_helpers) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->labml_helpers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->labml_helpers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->labml_helpers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->labml_helpers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->labml_helpers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->labml_helpers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->labml_helpers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->labml_helpers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->labml_helpers) (1.3.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->labml>=0.4.158->labml_helpers) (4.0.12)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->labml_helpers) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->labml>=0.4.158->labml_helpers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->labml>=0.4.158->labml_helpers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->labml>=0.4.158->labml_helpers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->labml>=0.4.158->labml_helpers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->labml>=0.4.158->labml_helpers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->labml>=0.4.158->labml_helpers) (2.4.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml_helpers) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->labml>=0.4.158->labml_helpers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->labml>=0.4.158->labml_helpers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->labml>=0.4.158->labml_helpers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->labml>=0.4.158->labml_helpers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->labml>=0.4.158->labml_helpers) (2024.2.0)\nDownloading labml_helpers-0.4.89-py3-none-any.whl (24 kB)\nDownloading labml-0.5.3-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, labml, labml_helpers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed labml-0.5.3 labml_helpers-0.4.89 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T10:32:51.810207Z","iopub.execute_input":"2025-07-06T10:32:51.811327Z","iopub.status.idle":"2025-07-06T10:32:51.822632Z","shell.execute_reply.started":"2025-07-06T10:32:51.811248Z","shell.execute_reply":"2025-07-06T10:32:51.821655Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"LabML is an open-source deep learning experimentation and monitoring library designed to help researchers and developers track, visualize, and organize their machine learning experiments with minimal overhead. At its core, LabML provides a lightweight framework that integrates seamlessly with PyTorch (and to some extent TensorFlow) and is structured around simplicity, clarity, and rapid iteration. One of its key components is the `labml_helpers` module, which provides abstractions like `Module`, `Tracker`, and `Monitors`, enabling users to define neural network components, monitor training metrics, and manage configurations in a highly readable and modular format. Unlike heavier platforms like MLflow or TensorBoard, LabML emphasizes minimal setup, requiring only a few lines of code to begin tracking training loss, accuracy, gradients, and learning rates. It also features a real-time web UI that can run locally, letting users visualize metrics and model behavior as training progresses. Additionally, LabML's logging system allows automatic capturing of hyperparameters, model structures, and performance metrics, making it ideal for rapid experimentation, reproducibility, and debugging. It’s particularly popular in educational settings and for researchers who want transparency and control without the complexity of heavyweight frameworks.\n","metadata":{}},{"cell_type":"markdown","source":"**self.hidden_lin = nn.Linear(hidden_size, 4 * hidden_size)**\n\n\nThis line creates a fully connected linear layer that takes in the previous hidden state hₜ₋₁ (with size hidden_size) and outputs a vector of size 4 × hidden_size.\n\nWhy 4×?\nBecause in an LSTM cell, we need to compute four separate vectors:\n\nInput gate iₜ – controls what new information to add to memory.\n\nForget gate fₜ – controls what to remove from memory.\n\nOutput gate oₜ – controls what to output as the hidden state.\n\nCandidate vector gₜ – proposed content to add to memory.\n\nSo instead of creating four separate Linear layers, this layer combines them into one operation that outputs all four vectors at once. Later, this combined vector is usually split into four parts internally.\nThis improves efficiency and keeps the code cleaner.\n\n\n**self.input_lin = nn.Linear(input_size, 4 * hidden_size, bias=False)**\n\nThis is another linear layer, similar to the above, but it acts on the current input xₜ.\n\nIt takes a vector of size input_size (the current input features),\n\nAnd maps it to a vector of size 4 × hidden_size (same purpose — computing iₜ, fₜ, oₜ, and gₜ),\n\nThis time, with no bias term (bias=False). That’s often done to avoid redundant biases when summing both input_lin and hidden_lin outputs later.","metadata":{}},{"cell_type":"code","source":"class LSTMCell(Module):\n    def __init__(self,input_size:int,hidden_size:int,layer_norm:bool=False):\n        super().__init__()\n        self.hidden_lin=nn.Linear(hidden_size,4*hidden_size)\n        self.input_lin=nn.Linear(input_size,4*hidden_size,bias=False)\n\n        if layer_norm:\n            self.layer_norm=nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(4)])\n            self.layer_norm_c=nn.LayerNorm(hidden_size)\n        else:\n            self.layer_norm=nn.ModuleList([nn.Identity() for _ in range(4)])\n            self.layer_norm_c=nn.Identity()\n            \n    def forward(self, x:torch.Tensor, h:torch.tensor, c:torch.tensor):\n        ifgo=self.hidden_lin(h) + self.input_lin(x)\n        ifgo=ifgo.chunk(4,dim=-1)\n\n        ifgo=[self.layer_norm[i](ifgo[i] for i in range(4))]\n        i,f,g,o=ifgo\n\n        c_next=torch.sigmoid(f)*c + torch.sigmoid(i)*torch.tanh(g)\n        h_next=torch.sigmoid(o)*torch.tanh(self.layer_norm_c(c_next))\n\n        return h_next,c_next\n    \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T10:32:59.221379Z","iopub.execute_input":"2025-07-06T10:32:59.222817Z","iopub.status.idle":"2025-07-06T10:32:59.233037Z","shell.execute_reply.started":"2025-07-06T10:32:59.222756Z","shell.execute_reply":"2025-07-06T10:32:59.231777Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class LSTM(Module):\n    def __init__(self,input_size: int,hidden_size: int,n_layers: int):\n        super().__init__()\n        self.n_layers=n_layers\n        self.hidden_size=hidden_size\n\n        self.cells = nn.ModuleList([LSTMCell(input_size,hidden_size)]+[LSTMCell(hidden_size,hidden_size) for _  in range(n_layers -1)])\n\n\n    def forward(self,x: torch.Tensor, state: Optional[Tuple[torch.Tensor,torch.Tensor]]=None):\n         n_steps,batch_size=x.shape[:2]\n\n         if state is None:\n             h=[x.new_zeros(batch_size,self.hidden_size) for _ in range(self.n_layers)]\n             c=[x.new_zeros(batch_size,self.hidden_size) for _ in range(self.n_layers)]\n         else:\n             (h,c)=state\n             h,c =list(torch.unbind(h)),list(torch.unbind(c))\n\n         out=[]\n         for t in range(n_steps):\n             inp=x[t]\n             for layer in range(self.n_layers):\n                 h[layer],c[layer]=self.cells[layer](inp,h[layer],c[layer])\n                 inp=h[layer]\n                 out.append(h[-1])\n         \n         out=torch.stack(out)\n         h=torch.stack(h)\n         c=torch.stack(c)\n\n         return out, (h,c)\n         \n                 \n         \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T10:33:18.356867Z","iopub.execute_input":"2025-07-06T10:33:18.357803Z","iopub.status.idle":"2025-07-06T10:33:18.368926Z","shell.execute_reply.started":"2025-07-06T10:33:18.357767Z","shell.execute_reply":"2025-07-06T10:33:18.367642Z"}},"outputs":[],"execution_count":9}]}